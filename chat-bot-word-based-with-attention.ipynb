{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# built-in module in Python that is used to inherit new features that will be available in the new Python versions\n\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\n\nfrom keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\nfrom keras.models import Model, load_model\n\nINPUT_LENGTH = 20\nOUTPUT_LENGTH = 20\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-16T14:12:38.876173Z","iopub.execute_input":"2022-10-16T14:12:38.876495Z","iopub.status.idle":"2022-10-16T14:12:38.891947Z","shell.execute_reply.started":"2022-10-16T14:12:38.876438Z","shell.execute_reply":"2022-10-16T14:12:38.891338Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['movie_conversations.txt', 'README.txt', 'chameleons.pdf', 'movie_titles_metadata.txt', 'movie_characters_metadata.txt', 'movie_lines.txt', '.DS_Store', 'raw_script_urls.txt']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Resources:\nhttps://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html","metadata":{"_uuid":"9c5007896f6a9aaf3395fc82447ca2bb5cc57b65"}},{"cell_type":"code","source":"# Load the data\nlines = open('../input/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\nconv_lines = open('../input/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-10-16T14:12:45.037642Z","iopub.execute_input":"2022-10-16T14:12:45.038158Z","iopub.status.idle":"2022-10-16T14:12:45.955552Z","shell.execute_reply.started":"2022-10-16T14:12:45.038076Z","shell.execute_reply":"2022-10-16T14:12:45.954538Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create a dictionary to map each line's id with its text\nid2line = {}\nfor line in lines:\n    _line = line.split(' +++$+++ ')\n    if len(_line) == 5:\n        id2line[_line[0]] = _line[4]","metadata":{"_uuid":"3983a4469fd5ed2f4b8db5a6cc41eda070c9142f","execution":{"iopub.status.busy":"2022-10-16T14:12:54.130944Z","iopub.execute_input":"2022-10-16T14:12:54.131293Z","iopub.status.idle":"2022-10-16T14:12:54.548870Z","shell.execute_reply.started":"2022-10-16T14:12:54.131227Z","shell.execute_reply":"2022-10-16T14:12:54.547929Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Create a list of all of the conversations' lines' ids.\nconvs = []\nfor line in conv_lines[:-1]:\n    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n    convs.append(_line.split(','))","metadata":{"_uuid":"260fe85783607a5f8190796b4415dee00cdeabc5","execution":{"iopub.status.busy":"2022-10-16T11:05:17.102930Z","iopub.execute_input":"2022-10-16T11:05:17.103552Z","iopub.status.idle":"2022-10-16T11:05:17.432320Z","shell.execute_reply.started":"2022-10-16T11:05:17.103499Z","shell.execute_reply":"2022-10-16T11:05:17.431051Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#id and conversation sample\nfor k in convs[300]:\n    print (k, id2line[k])","metadata":{"_uuid":"c7ac8d38b75b1101165c37cdab3fca42597eb4d7","execution":{"iopub.status.busy":"2022-10-16T11:05:19.611415Z","iopub.execute_input":"2022-10-16T11:05:19.611775Z","iopub.status.idle":"2022-10-16T11:05:19.618371Z","shell.execute_reply.started":"2022-10-16T11:05:19.611706Z","shell.execute_reply":"2022-10-16T11:05:19.617277Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"L3490 That's what he did to me.  He put cigarettes out on me.\nL3491 Your father put cigarettes out on you?\nL3492 Out on my back when I was a small boy.\nL3493 Can I see your back?\n","output_type":"stream"}]},{"cell_type":"code","source":"# Sort the sentences into questions (inputs) and answers (targets)\nquestions = []\nanswers = []\nfor conv in convs:\n    for i in range(len(conv)-1):\n        questions.append(id2line[conv[i]])\n        answers.append(id2line[conv[i+1]])\n        \n# Compare lengths of questions and answers\nprint(len(questions))\nprint(len(answers))","metadata":{"_uuid":"a3142e705c67fd80837a998a7884dd584c30f57c","execution":{"iopub.status.busy":"2022-10-16T11:05:27.605383Z","iopub.execute_input":"2022-10-16T11:05:27.605783Z","iopub.status.idle":"2022-10-16T11:05:27.882282Z","shell.execute_reply.started":"2022-10-16T11:05:27.605690Z","shell.execute_reply":"2022-10-16T11:05:27.881402Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"221616\n221616\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_text(text):\n    '''Clean text by removing unnecessary characters and altering the format of words.'''\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"that is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"'bout\", \"about\", text)\n    text = re.sub(r\"'til\", \"until\", text)\n    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n    text = \" \".join(text.split())\n    return text","metadata":{"_uuid":"85f3e1713f0620f066f5a5ee34b31c9a73d768fa","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean the data\nclean_questions = []\nfor question in questions:\n    clean_questions.append(clean_text(question))\nclean_answers = []    \nfor answer in answers:\n    clean_answers.append(clean_text(answer))","metadata":{"_uuid":"162b9f09f2e3e50ed92f157a5ce0faf3bb0d0019","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the length of sentences (not using nltk due to processing speed)\nlengths = []\n# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\nfor question in clean_questions:\n    lengths.append(len(question.split()))\nfor answer in clean_answers:\n    lengths.append(len(answer.split()))\n# Create a dataframe so that the values can be inspected\nlengths = pd.DataFrame(lengths, columns=['counts'])\nprint(np.percentile(lengths, 80))\nprint(np.percentile(lengths, 85))\nprint(np.percentile(lengths, 90))\nprint(np.percentile(lengths, 95))","metadata":{"_uuid":"b1df72b880bbf79a8043ae3b608d37e89b5807e9","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove questions and answers that are shorter than 1 word and longer than 20 words.\nmin_line_length = 2\nmax_line_length = 20\n\n# Filter out the questions that are too short/long\nshort_questions_temp = []\nshort_answers_temp = []\n\nfor i, question in enumerate(clean_questions):\n    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n        short_questions_temp.append(question)\n        short_answers_temp.append(clean_answers[i])\n\n# Filter out the answers that are too short/long\nshort_questions = []\nshort_answers = []\n\nfor i, answer in enumerate(short_answers_temp):\n    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n        short_answers.append(answer)\n        short_questions.append(short_questions_temp[i])\n        \nprint(len(short_questions))\nprint(len(short_answers))","metadata":{"_uuid":"0035c49ba3da36e903a600b22c737e6a5c34de6e","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = np.random.randint(1,len(short_questions))\n\nfor i in range(r, r+3):\n    print(short_questions[i])\n    print(short_answers[i])\n    print()","metadata":{"_uuid":"85389732fd564d7b160339bb26d0d1b5dea41fe7","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1  Preprocessing for word based model","metadata":{"_uuid":"5825a5967e5a04f13b0a755e8ba304c419af4b33"}},{"cell_type":"code","source":"#choosing number of samples\nnum_samples = 30000  # Number of samples to train on.\nshort_questions = short_questions[:num_samples]\nshort_answers = short_answers[:num_samples]\n#tokenizing the qns and answers\nshort_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\nshort_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]","metadata":{"_uuid":"106248b9b0ac7499c2b34cc3ae1c20a2e0ea41c4","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train-validation split\ndata_size = len(short_questions_tok)\n\n# We will use the first 0-80th %-tile (80%) of data for the training\ntraining_input  = short_questions_tok[:round(data_size*(80/100))]\ntraining_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\ntraining_output = short_answers_tok[:round(data_size*(80/100))]\n\n# We will use the remaining for validation\nvalidation_input = short_questions_tok[round(data_size*(80/100)):]\nvalidation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\nvalidation_output = short_answers_tok[round(data_size*(80/100)):]\n\nprint('training size', len(training_input))\nprint('validation size', len(validation_input))","metadata":{"_uuid":"8b37316d3b68626ed388bbb73863364608027796","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2  Word en/decoding dictionaries","metadata":{"_uuid":"a3aacc0315837faf77599da087f8b3a52d217859"}},{"cell_type":"code","source":"# Create a dictionary for the frequency of the vocabulary\n# Create \nvocab = {}\nfor question in short_questions_tok:\n    for word in question:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1\n\nfor answer in short_answers_tok:\n    for word in answer:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1            ","metadata":{"_uuid":"51918567f4f43ae13a8d58a00bda15b1d508b191","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove rare words from the vocabulary.\n# We will aim to replace fewer than 5% of words with <UNK>\n# You will see this ratio soon.\nthreshold = 15\ncount = 0\nfor k,v in vocab.items():\n    if v >= threshold:\n        count += 1","metadata":{"_uuid":"30d25056e8f62453bcde0a16c8a7933275f431c9","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Size of total vocab:\", len(vocab))\nprint(\"Size of vocab we will use:\", count)","metadata":{"_uuid":"4c980b035173d12d2bf06dafd6410eb5da6023b2","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we will create dictionaries to provide a unique integer for each word.\nWORD_CODE_START = 1\nWORD_CODE_PADDING = 0\n\n\nword_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\nencoding = {}\ndecoding = {1: 'START'}\nfor word, count in vocab.items():\n    if count >= threshold: #get vocabularies that appear above threshold count\n        encoding[word] = word_num \n        decoding[word_num ] = word\n        word_num += 1\n\nprint(\"No. of vocab used:\", word_num)","metadata":{"_uuid":"d62a629773e34bb7bc1a12508cce820c0e47962d","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#include unknown token for words not in dictionary\ndecoding[len(encoding)+2] = '<UNK>'\nencoding['<UNK>'] = len(encoding)+2","metadata":{"_uuid":"de2e46d3826def6b6c94a9827c32118d555fae2e","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_size = word_num+1\ndict_size","metadata":{"_uuid":"bccd51e8730e7c3da1234af9480da767f400c230","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3  Vectorizing dataset","metadata":{"_uuid":"7ea3c32a6180ee8fea8b1490b40351897cc426e6"}},{"cell_type":"code","source":"def transform(encoding, data, vector_size=20):\n    \"\"\"\n    :param encoding: encoding dict built by build_word_encoding()\n    :param data: list of strings\n    :param vector_size: size of each encoded vector\n    \"\"\"\n    transformed_data = np.zeros(shape=(len(data), vector_size))\n    for i in range(len(data)):\n        for j in range(min(len(data[i]), vector_size)):\n            try:\n                transformed_data[i][j] = encoding[data[i][j]]\n            except:\n                transformed_data[i][j] = encoding['<UNK>']\n    return transformed_data","metadata":{"_uuid":"4de33f91476bd6c50cf7c9322f273079783ceec2","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoding training set\nencoded_training_input = transform(\n    encoding, training_input, vector_size=INPUT_LENGTH)\nencoded_training_output = transform(\n    encoding, training_output, vector_size=OUTPUT_LENGTH)\n\nprint('encoded_training_input', encoded_training_input.shape)\nprint('encoded_training_output', encoded_training_output.shape)","metadata":{"_uuid":"f13007fbe8661c4c2ea4c35fd51b86c4fc220efb","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoding validation set\nencoded_validation_input = transform(\n    encoding, validation_input, vector_size=INPUT_LENGTH)\nencoded_validation_output = transform(\n    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n\nprint('encoded_validation_input', encoded_validation_input.shape)\nprint('encoded_validation_output', encoded_validation_output.shape)","metadata":{"_uuid":"cbbf370a5d0640a4dea6b6d9237a5917978b92a8","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2  Model Building\n### 2.1  Sequence-to-Sequence in Keras","metadata":{"_uuid":"b52f0368fecf8ac79903927799fe67c104d8e518"}},{"cell_type":"code","source":"import tensorflow as tf\ntf.keras.backend.clear_session()","metadata":{"_uuid":"e8b5afc77468e4e263b3fc7ff62a2e6952a80e97","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_LENGTH = 20\nOUTPUT_LENGTH = 20\n\nencoder_input = Input(shape=(INPUT_LENGTH,))\ndecoder_input = Input(shape=(OUTPUT_LENGTH,))","metadata":{"_uuid":"306b1fb012febd74dd40840c0588bdd8503ba65b","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import SimpleRNN\n\nencoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\nencoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\nencoder_last = encoder[:,-1,:]\n\nprint('encoder', encoder)\nprint('encoder_last', encoder_last)\n\ndecoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\ndecoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n\nprint('decoder', decoder)\n\n# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)","metadata":{"_uuid":"d91367ec6c706059c4b520011fdec6e1051db38d","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2  Attention Mechanism\nReference: Effective Approaches to Attention-based Neural Machine Translation's Global Attention with Dot-based scoring function (Section 3, 3.1) https://arxiv.org/pdf/1508.04025.pdf","metadata":{"_uuid":"2feb58fa60a8ff335b218114808aa2d3e4d37461"}},{"cell_type":"code","source":"from keras.layers import Activation, dot, concatenate\n\n# Equation (7) with 'dot' score from Section 3.1 in the paper.\n# Note that we reuse Softmax-activation layer instead of writing tensor calculation\nattention = dot([decoder, encoder], axes=[2, 2])\nattention = Activation('softmax', name='attention')(attention)\nprint('attention', attention)\n\ncontext = dot([attention, encoder], axes=[2,1])\nprint('context', context)\n\ndecoder_combined_context = concatenate([context, decoder])\nprint('decoder_combined_context', decoder_combined_context)\n\n# Has another weight + tanh layer as described in equation (5) of the paper\noutput = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\noutput = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\nprint('output', output)","metadata":{"_uuid":"89c5ce99b6b7e42bf02559816dc56cdc9a29423e","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\nmodel.summary()","metadata":{"_uuid":"257dcf12770f8928f91754044d3efc5aff1ec296","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_encoder_input = encoded_training_input\ntraining_decoder_input = np.zeros_like(encoded_training_output)\ntraining_decoder_input[:, 1:] = encoded_training_output[:,:-1]\ntraining_decoder_input[:, 0] = WORD_CODE_START\ntraining_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n\nvalidation_encoder_input = encoded_validation_input\nvalidation_decoder_input = np.zeros_like(encoded_validation_output)\nvalidation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\nvalidation_decoder_input[:, 0] = WORD_CODE_START\nvalidation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]","metadata":{"_uuid":"09b61276a10c14838eb09db124a229ac58c61133","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n          #validation_split=0.05,\n          batch_size=64, epochs=100)\n\nmodel.save('model_attention.h5')","metadata":{"_uuid":"3d43e174878dfbef64900b7f043d7bb41f458309","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Model testing","metadata":{"_uuid":"874f0398f10f48cce993bcd9aa7b5e0644b6f292","trusted":true}},{"cell_type":"code","source":"def prediction(raw_input):\n    clean_input = clean_text(raw_input)\n    input_tok = [nltk.word_tokenize(clean_input)]\n    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n    encoder_input = transform(encoding, input_tok, 20)\n    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n    decoder_input[:,0] = WORD_CODE_START\n    for i in range(1, OUTPUT_LENGTH):\n        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n        decoder_input[:,i] = output[:,i]\n    return output\n\ndef decode(decoding, vector):\n    \"\"\"\n    :param decoding: decoding dict built by word encoding\n    :param vector: an encoded vector\n    \"\"\"\n    text = ''\n    for i in vector:\n        if i == 0:\n            break\n        text += ' '\n        text += decoding[i]\n    return text","metadata":{"_uuid":"d0e3217f3d250c253dd457f192f54f8cd67c630f","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(20):\n    seq_index = np.random.randint(1, len(short_questions))\n    output = prediction(short_questions[seq_index])\n    print ('Q:', short_questions[seq_index])\n    print ('A:', decode(decoding, output[0]))","metadata":{"_uuid":"16e6ece57e4c02321c58176e5471f37ad1170837","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_input = input()\noutput = prediction(raw_input)\nprint (decode(decoding, output[0]))","metadata":{"_uuid":"0d5742477af42914841bb341532f7e9e01c15815","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c8909443bde5f6195e9a2983cdc633b9081c893d","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}